{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model implementation 01\n",
    "\n",
    "This notebook is a first pass at reproducing the Wassertain GAN model used \n",
    "in\n",
    "\n",
    "\n",
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model makes use of residual blocks in both predictor and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual blocks used in generator and discriminator\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, d_in, d_out, relu): #pick nn.ReLU for generator, nn.LeakyRelu for discriminator\n",
    "        super().__init__()\n",
    "        self.relu_1 = relu()\n",
    "        self.conv_1 = nn.Conv1d(in_channels=d_in, out_channels=d_out, kernel_size=5, padding = 2)\n",
    "        self.relu_2 = relu()\n",
    "        self.conv_2 = nn.Conv1d(in_channels=d_in, out_channels=d_out, kernel_size=5, padding = 2)\n",
    "        # bias not needed, already present by default in Conv1d()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.relu_1(x)\n",
    "        y = self.conv_1(y)\n",
    "        y = self.relu_2(y)\n",
    "        y = self.conv_2(y)\n",
    "        return x + (0.3 * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test this class with a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualBlock(\n",
      "  (relu_1): ReLU()\n",
      "  (conv_1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (relu_2): ReLU()\n",
      "  (conv_2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "resblock = ResidualBlock(100, 100, nn.ReLU)\n",
    "print(resblock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2654,  0.9001,  0.9767,  ...,  0.7684, -0.6504, -0.2017],\n",
       "         [ 0.4391, -0.5020, -1.0044,  ..., -0.7929,  1.2517,  0.0854],\n",
       "         [-0.8011, -1.2852, -0.9045,  ...,  0.2945,  0.3540,  0.1076],\n",
       "         ...,\n",
       "         [ 0.9124,  2.4432, -0.6559,  ..., -1.5265,  0.4473, -0.2616],\n",
       "         [-0.2717,  0.5713,  0.6560,  ...,  2.0048, -1.6967, -1.1009],\n",
       "         [ 0.2134,  0.3010,  0.1197,  ...,  0.9366,  0.5990, -0.9680]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 100, 100)\n",
    "resblock(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify blocks work on intended input shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test input linear layer layout, with an input from latent space $Z$, then feed into resblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(100)\n",
    "lin = nn.Linear(100, 100*50) # for sequence of 50 nucleotides\n",
    "\n",
    "x = lin(z) \n",
    "x = torch.reshape(x, (1,50,100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 100])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin(z).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(100)\n",
    "lin = nn.Linear(100, 100*50) # for sequence of 50 nucleotides\n",
    "\n",
    "x = lin(z) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, seq_len, batch_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.linear = nn.Linear(100, 100*seq_len)\n",
    "        self.res = []\n",
    "        for i in range(5):\n",
    "            self.res.append(ResidualBlock(100, 100, relu=nn.ReLU))\n",
    "        self.conv_f = nn.Conv1d(in_channels=100, out_channels=4, kernel_size=1)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def forward(self,z):\n",
    "        y = self.linear(z)\n",
    "        y = torch.reshape(y, (self.batch_size, 100, self.seq_len))\n",
    "        for i in range(5):\n",
    "            y = self.res[i](y)\n",
    "        y = self.conv_f(y)\n",
    "        y = F.softmax(y, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test generator on a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 50])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.randn(100)\n",
    "generator = Generator(50, 1)\n",
    "x = generator(z)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Cond1d(in_channels=4, out_channels=100)\n",
    "    def forward(self, z):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient penalty used in WGAN-WP architecture [GitHub source](https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/wgan_gp/wgan_gp.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
