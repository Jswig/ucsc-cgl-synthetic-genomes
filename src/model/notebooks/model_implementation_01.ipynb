{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model implementation 01\n",
    "\n",
    "This notebook is a first pass at implementing in PyTorch the Wassertain GAN model used in \n",
    "[Generating and designing DNA with deep generative models](https://arxiv.org/abs/1712.06148) (Killoran and al, 2017)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model makes use of residual blocks in both predictor and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual blocks used in generator and discriminator\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, d_in, d_out, relu): #pick nn.ReLU for generator, nn.LeakyRelu for discriminator\n",
    "        super().__init__()\n",
    "        self.relu_1 = relu()\n",
    "        self.conv_1 = nn.Conv1d(in_channels=d_in, out_channels=d_out, kernel_size=5, padding = 2)\n",
    "        self.relu_2 = relu()\n",
    "        self.conv_2 = nn.Conv1d(in_channels=d_in, out_channels=d_out, kernel_size=5, padding = 2)\n",
    "        # bias not needed, already present by default in Conv1d()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.relu_1(x)\n",
    "        y = self.conv_1(y)\n",
    "        y = self.relu_2(y)\n",
    "        y = self.conv_2(y)\n",
    "        return x + (0.3 * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test this class with a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualBlock(\n",
      "  (relu_1): ReLU()\n",
      "  (conv_1): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (relu_2): ReLU()\n",
      "  (conv_2): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "resblock = ResidualBlock(100, 100, nn.ReLU)\n",
    "print(resblock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.1803,  1.4953,  1.8549,  ..., -1.3184,  0.8193, -0.3785],\n",
       "         [-0.8168, -0.5969, -0.1555,  ..., -0.1371,  1.0691, -2.5855],\n",
       "         [-1.0546,  1.9645,  0.5384,  ...,  0.3895,  1.5450, -0.4745],\n",
       "         ...,\n",
       "         [ 0.3187, -0.6233,  1.5000,  ...,  0.4980, -1.0958,  1.4441],\n",
       "         [ 0.4771, -0.4912,  0.3552,  ...,  0.5405,  1.1066,  0.0172],\n",
       "         [ 0.3780,  1.8369, -0.2172,  ..., -0.9122,  0.2721,  0.5916]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 100, 100)\n",
    "resblock(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify blocks work on intended input shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test input linear layer layout, with an input from latent space $Z$, then feed into resblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(100)\n",
    "lin = nn.Linear(100, 100*50) # for sequence of 50 nucleotides\n",
    "\n",
    "x = lin(z) \n",
    "x = torch.reshape(x, (1,50,100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 100])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin(z).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(100)\n",
    "lin = nn.Linear(100, 100*50) # for sequence of 50 nucleotides\n",
    "\n",
    "x = lin(z) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, seq_len, batch_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.linear_in = nn.Linear(in_features=100, out_features=100*seq_len)\n",
    "        self.resblocks = []\n",
    "        for _ in range(5):\n",
    "            self.resblocks.append(ResidualBlock(100, 100, relu=nn.ReLU))\n",
    "        self.conv_out = nn.Conv1d(in_channels=100, out_channels=4, kernel_size=1)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def forward(self,z):\n",
    "        y = self.linear_in(z)\n",
    "        y = torch.reshape(y, (self.batch_size, 100, self.seq_len))\n",
    "        for i in range(5):\n",
    "            y = self.resblocks[i](y)\n",
    "        y = self.conv_out(y)\n",
    "        y = F.softmax(y, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test generator on a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 50])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.randn(100)\n",
    "generator = Generator(50, 1)\n",
    "x = generator(z)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2510, 0.2497, 0.1958, 0.1733, 0.1547, 0.2807, 0.1279, 0.5038,\n",
       "          0.4120, 0.1492, 0.2069, 0.3457, 0.1523, 0.2672, 0.2112, 0.3845,\n",
       "          0.2338, 0.2684, 0.2222, 0.2089, 0.2416, 0.2203, 0.1738, 0.2467,\n",
       "          0.2477, 0.2930, 0.2330, 0.1932, 0.1619, 0.2039, 0.3279, 0.1746,\n",
       "          0.3710, 0.3533, 0.2747, 0.2756, 0.2343, 0.2874, 0.2969, 0.3586,\n",
       "          0.2194, 0.1976, 0.1705, 0.2653, 0.3341, 0.2387, 0.2466, 0.2969,\n",
       "          0.3488, 0.2826],\n",
       "         [0.1645, 0.2566, 0.2439, 0.3421, 0.2916, 0.2685, 0.1815, 0.1640,\n",
       "          0.2284, 0.1609, 0.4385, 0.1738, 0.5587, 0.2149, 0.2189, 0.1339,\n",
       "          0.2119, 0.1712, 0.3360, 0.2256, 0.2136, 0.1941, 0.3016, 0.1888,\n",
       "          0.2081, 0.2794, 0.1971, 0.3620, 0.1995, 0.1984, 0.1607, 0.2298,\n",
       "          0.1078, 0.1560, 0.2047, 0.2279, 0.1532, 0.1333, 0.1380, 0.2796,\n",
       "          0.1931, 0.3692, 0.2212, 0.2475, 0.1293, 0.1975, 0.1934, 0.2848,\n",
       "          0.1748, 0.1442],\n",
       "         [0.3368, 0.2439, 0.1729, 0.3146, 0.3277, 0.2440, 0.2029, 0.1588,\n",
       "          0.2192, 0.1342, 0.1262, 0.3125, 0.1811, 0.2772, 0.2334, 0.3003,\n",
       "          0.4355, 0.2998, 0.3120, 0.2177, 0.3157, 0.3412, 0.4355, 0.2061,\n",
       "          0.3731, 0.2086, 0.3678, 0.2628, 0.3990, 0.3992, 0.2862, 0.3201,\n",
       "          0.2503, 0.2492, 0.3405, 0.3360, 0.3024, 0.3570, 0.3420, 0.2514,\n",
       "          0.2885, 0.2775, 0.3530, 0.2762, 0.2973, 0.4560, 0.4303, 0.2251,\n",
       "          0.1257, 0.3053],\n",
       "         [0.2478, 0.2498, 0.3874, 0.1700, 0.2260, 0.2068, 0.4878, 0.1733,\n",
       "          0.1404, 0.5557, 0.2284, 0.1680, 0.1079, 0.2407, 0.3365, 0.1813,\n",
       "          0.1188, 0.2606, 0.1298, 0.3478, 0.2290, 0.2444, 0.0891, 0.3584,\n",
       "          0.1711, 0.2190, 0.2021, 0.1819, 0.2396, 0.1985, 0.2252, 0.2755,\n",
       "          0.2709, 0.2415, 0.1801, 0.1605, 0.3101, 0.2223, 0.2231, 0.1104,\n",
       "          0.2990, 0.1557, 0.2553, 0.2110, 0.2393, 0.1078, 0.1297, 0.1932,\n",
       "          0.3507, 0.2680]]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, seq_len, batch_size):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv1d(in_channels=4, out_channels=100, kernel_size=1)\n",
    "        self.resblocks = []\n",
    "        for _ in range(5):\n",
    "            self.resblocks.append(ResidualBlock(100, 100, relu=nn.ReLU))\n",
    "        self.linear_out = nn.Linear(in_features=100*seq_len, out_features=1)\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.conv_in(x)\n",
    "        for i in range(5):\n",
    "            y = self.resblocks[i](y)\n",
    "        y = torch.reshape(y, (self.batch_size, 100*self.seq_len))\n",
    "        y = self.linear_out(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple test on the previous output of the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator(50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = discriminator(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient penalty used in WGAN-WP architecture [GitHub source](https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/wgan_gp/wgan_gp.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
